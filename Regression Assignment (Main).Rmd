---
title: "MAT3024 Regression Analysis Assignment"
author: "Group 1"
date: "`r Sys.Date()`"
output: rmdformats::readthedown
---

# Group Members

Chong Kai Yuan (21081609)\
Ethan Lee Jia Hua (21089750)\
Daniel Chin Wei Jian (20095204)\
Darren Yap Yee Shern (21001235)\
Harreesh Dev Chandrasager (20083200)\
Nur' Aliah Syamimi Binti Muhammad Sazali (21081765)

<br>

# Introduction & Problem Statement

## Predicting Wine Quality Using Regression Analysis

Wine quality assessment is a crucial aspect of the wine industry. The ability to predict wine quality based on various chemical properties can significantly enhance decision-making processes in viticulture. Chemical analysis has long been an established method for determining wine quality, with factors such as acidity, sugar content, and alcohol levels known to influence taste and overall consumer satisfaction (Marianthi Basalekou et al., 2023).

Consumers rely heavily on quality ratings when selecting wines. Accurate predictions of wine quality enable producers to better meet consumer expectations, resulting in higher satisfaction and repeat purchases (Corduas et al., 2013). This study aims to identify and quantify the impact of different physicochemical properties on the quality ratings of white Vinho Verde wines.

## Determining Key Physicochemical Factors Influencing Wine Quality

Our primary objective is to create a regression model that highlights the most significant factors contributing to wine quality. The analysis will focus on identifying the key physicochemical properties that influence wine quality and quantifying their impact. As such, our task will help winemakers optimize their production processes and improve the quality of their wines.

<br>

# Data Processing

Load the use of all relevant packages used under this assignment

```{r message=FALSE, warning=FALSE, results = "hold"}

library(car)
library(MASS)
library(dplyr)
library(olsrr)
library(leaps)
library(psych)
library(moments)
library(ggplot2)
library(pastecs)
library(corrplot)
library(tidyverse)
library(AICcmodavg)
library(rmdformats)
library(kableExtra)
library(ggThemeAssist)
options(scipen=10)

```

Import dataset into R Studio

```{r, results = "hold"}

Dataset<-read.csv("winequality-white.csv", sep =";")
str(Dataset)

```

# Dataset Description: Wine Quality Dataset

The dataset provided is considered for *vinho verde*, a unique product from Minho from the northwest region in Portugal and was collected from May (2004) to February (2007). Note that both red and white wine datasets were available, but the white wine dataset was chosen for analysis.

![**Figure 1:** Vinho Verde White Wine](images/Vinho-Verde.jpg)

<br>

Variables Involved under considered dataset:

1.  Sulphates (g(potassium sulphate)/dm^3^)
2.  Chlorides (g(sodium chloride)/dm^3^)
3.  Volatile acidity (g(acetic acid)/dm^3^)
4.  Fixed acidity (g(tartaric acid)/dm^3^)
5.  Total sulfur dioxide (mg/dm^3^)
6.  Free sulfur dioxide (mg/dm^3^)
7.  Residual sugar (g/dm^3^)
8.  Citric acid (g/dm^3^)
9.  Density (g/cm^3^)
10. Alcohol (vol.%)
11. pH level
12. Quality

This Dataset consists of 4898 observations based on 12 variables

Data source: [Wine Quality Dataset](https://www.kaggle.com/datasets/yasserh/wine-quality-dataset)

<br>

# Preliminary Regression Metrics and Diagnostic Measures

Here we provide a summary of all the key regressionary metrics used during our analysis.

1.  Akaike Information Criterion ($AIC$) : measures relative and estimated quality of different statistical models; a lower value indicates a better-fitting model

2.  Bayesian Information Criterion ($BIC$) : similar to AIC but with stricter penalty in regards to the number of parameters being fitted in the model; a lower value indicates a better-fitting model

3.  R-squared ($R^2$) : statistical measure representing total proportion of variance explained by the regressor variables; values range from 0 to 1 with higher values indicating a higher quality model

4.  Residual Standard Error ($s$) : measures the standard deviation of the residuals in a regression model; a lower value indicates smaller residuals and thus indicates a better fitted model

5.  Variation Inflation Factor ($VIF$) : indicates how much the variance of the regression coefficients are inflated due to multicollinearity issues; values above 5 are considered as problematic and indicates the predictor is highly collinear

6.  Ols Mallows Cp ($Cp$) : measures the quality of the fitted model while also considering its overall complexity; values close to the number of predictors plus one suggests a good model

<br>

# Data Analysis

## Descriptive Statistics

```{r, results = "hold"}

round(stat.desc(Dataset),2)

```

**INTERPRETATION**

From the complete summary of the dataset, several key observations emerge regarding specific variables:

1.  Citric Acid: Among all regressors, citric acid exhibited the smallest minimum value across all observations.

2.  Total Sulfur Dioxide: This variable had the highest maximum value, the widest range, and the largest standard error of the mean.

Interestingly, when applying linear modeling techniques to our updated full model, both citric acid and total sulfur dioxide were found to be statistically insignificant. This lack of significance may be partially attributable to the distinctive properties they exhibit, as mentioned above.

<br>

## Visualizing the dependent variable

```{r message=FALSE, warning=FALSE, results="hold"}

# construct histogram

hist_data <- hist(Dataset$quality, breaks = seq(3, 10, by = 1), plot = FALSE, right = FALSE)

hist_df <- data.frame(midpoints = hist_data$mids, counts = hist_data$counts)


ggplot(Dataset, aes(x = quality)) +
  geom_histogram(breaks = seq(3, 9, by = 1), fill = "grey", color = "white", closed = "left", boundary = 3) +
  geom_line(data = hist_df, aes(x = midpoints, y = counts), color = "red", size = 1) +
  geom_point(data = hist_df, aes(x = midpoints, y = counts), color = "red", size = 2) +
  labs(title = "Figure 2: Histogram of Wine Quality",
       x = "Quality of Wine",
       y = "Frequency") + 
  theme(plot.title = element_text(hjust=0.5)) +
  scale_x_continuous(breaks = seq(3, 9, by = 1)) # Customize x-axis labels to show end values

skewness_value<-skewness(Dataset$quality)
skewness_value

```

**INTERPRETATION**

-   The histogram shows a roughly bell-shaped curve, which suggests that the data might be approximately normally distributed. Many statistical methods, including linear regression, assume that the residuals of the model (differences between observed and predicted values) are normally distributed. Thus, A bell-shaped histogram for quality suggests that the response variable itself is normally distributed, which is a good starting point for meeting this assumption.

-   The mean quality score is 5.88 as identified when using the stat.desc() function, this is consistent with the peak of the histogram around 6. This suggests that the mean is a good measure of central tendency for this data.

<br>

## Fitting the Full Model

Here, we observe the model using the entirety of the wine dataset in terms of its AIC, BIC, Residual standard error and its R^2^. Additionally, we investigate the whether each of the predictor variables appear to be statistically significant in predicting our dependent variable

```{r, results = "hold"}

# fitting full model

full_model<-lm(quality~.,data=Dataset)
summary(full_model)

# extracting and finding relevant metrics

Residual_standard_error_full_model<-summary(full_model)$sigma
R_squared_full_model<-summary(full_model)$r.squared
full_model_AIC<-AIC(full_model)
full_model_BIC<-BIC(full_model)

# constructing table

table_1 <- data.frame(
  Metrics = c("AIC", "BIC", "R-squared", "Residual standard error"),
  full_model = c(full_model_AIC, full_model_BIC, R_squared_full_model, Residual_standard_error_full_model))

# formatting table

table_1$full_model <- format(table_1$full_model, digits = 5)

# displaying table

knitr::kable(table_1, caption = "Table 1: Metrics from the Full Model") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)



```

**Observation of Full Model**

From the output of the summary of the dataset, we can observe that three predictor variables can be highlighted and appears to not be statistically significant. Those variables being:

1.  Citric Acid (g/dm^3^)

2.  Total sulfur dioxide (mg/dm^3^)

3.  Chlorides (g(sodium chloride)/dm^3^)

From here we can start making the hypothesis that when performing the variable selection process to find our "best" regression model, it is more likely than not that one or several of these variables will likely not be included under the fitted model.

<br>

## Investigating Problematic Data Points

Before we proceed into building our regression models, it is crucial to ensure the reliability and accuracy of our current dataset by investigating any potentially problematic observation points. In here, we decided to take into account the following metrics:

1)  Leverage (*h~ii~*) - may affect regression coefficients

2)  Cook's Distance (*D~i~*) - to observe influential points

3)  Studentized Residuals (*r~i~*) - to observe potential outliers

Observation points that exceed tolerated thresholds in these diagnostic measures will then be considered as problematic. Consequently, an updated full model will be created after removing these problematic points, ensuring a more robust and accurate predictive model.

<br>

## Visualizing Model using Diagnostic Plots

```{r, results = "hold"}

par(mfrow=c(1,2))
plot(full_model,which=2, main="Figure 3(a)") # QQ Residuals
plot(full_model,which=5, main="Figure 3(b)") # Residuals vs Leverage

```

**INTERPRETATION**

QQ Residuals: The lines curve at the ends, implying that under the full model, the residuals have heavier tails than a normal distribution, this acts as an indicator for the presence of outliers in the dataset.

Residuals vs Leverage: Presence of observations clearly outside the Cook's distance contour lines, indicating presence of influential points within the Dataset.

Now, we will make use of the following thresholds in deciding which observations points are considered problematic which should be removed from our dataset.

|   Diagnostic Value Tested    | Tolerated Threshold |
|:----------------------------:|:-------------------:|
| Studentized Residuals $(ri)$ |        $< 3$        |
|    Cook's Distance $(Di)$    |       $< 4/n$       |
|       Leverage $(hii)$       |      $< 2p/n$       |

: **Table 2**: Diagnostics values and their associated thresholds

Under different applications, the threshold for the cook's distance may also be $< 1$, however, we decided to use the threshold of $< 4/n$ instead being that in this way, the number of observations in the dataset is taken into account when performing our investigation of problematic observation points.

```{r, results = "hold"}

# studentized residuals 

studentized_residuals_full_model<-studres(full_model)

# cooks distance 

cd_full_model<-cooks.distance(full_model)

# leverage

hat_values_full_model<-hatvalues(full_model)

# plot for visualization

par(mfrow=c(1,3))

plot(studentized_residuals_full_model, main="Figure 4(a): Studentized residuals \nvisualization", ylab="Studentized Residuals",xlab="Observation point")
abline(h=3,col="red")
abline(h=-3,col="red")

plot(cd_full_model, main="Figure 4(b): Cook's distance \nvisualization", ylab="cooks distance",xlab="Observation point")
abline(h = 4/(nrow(Dataset)), col = "red")

plot(hat_values_full_model, main="Figure 4(c): Leverage \nvisualization",ylab="leverage", xlab="Observation point")
abline(h= 2 * (length(full_model$coefficients)) / nrow(Dataset),col ="red")

```

```{r, results = "hold"}

# indicate all flagged points that need to be removed

high_studentized_residual_points<-which(studentized_residuals_full_model>(abs(3)))
high_leverage_points<-which(hat_values_full_model>(2*(length(full_model$coefficients))/nrow(Dataset)))
high_cooks_distance_points<-which(cd_full_model>(4/nrow(Dataset)))

all_potential_issues<-unique(c(high_studentized_residual_points,high_leverage_points,high_cooks_distance_points))

# create updated full model

updated_Dataset<-Dataset[-all_potential_issues, ]

```

<br>

## Fitting Updated Full Model

```{r}

# fitting updated full model and observing observations

updated_full_model<-lm(quality~.,data=updated_Dataset)
str(updated_Dataset)

```

```{r, results = "hold"}

# inbestigating updated full model

summary(updated_full_model)

# extracting and finding relevant metrics

Residual_standard_error_updated_full_model<-summary(updated_full_model)$sigma
R_squared_updated_full_model<-summary(updated_full_model)$r.squared
updated_full_model_AIC<-AIC(updated_full_model)
updated_full_model_BIC<-BIC(updated_full_model)

# constructing the table

table_2 <- data.frame(
  Metrics = c("AIC", "BIC", "R-squared", "Residual standard error"),
  full_model = c(full_model_AIC, full_model_BIC, R_squared_full_model, Residual_standard_error_full_model),
  updated_full_model = c(updated_full_model_AIC, updated_full_model_BIC, R_squared_updated_full_model, Residual_standard_error_updated_full_model))

# formatting table

table_2$full_model <- format(table_2$full_model, digits = 5)
table_2$updated_full_model <- format(table_2$updated_full_model, digits = 5)

# display updated table

knitr::kable(table_2, caption = "Table 3: Comparison of Original vs Updated Full Model") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)


```

**INTERPRETATION**

By inspection, the new dataset has 4470 observations which shows that a total of 428 observation points have been removed. At first glance, we can see that after removing the problematic points, the updated model evidently exhibit better model properties. It has a lower AIC and BIC value, which indicates that the model has an improved fit and better performance relative to the previous model. Additionally, it has a smaller residual standard error, which shows that the predictions of the model are more accurate while having less variability. The R-squared value is also higher, indicating that a greater proportion of the variance in the dependent variable is explained by the independent variables in the model which is ideal for a regression model.

Additionally, from the output of the summary of the updated dataset, we can see that now only two of the predictor variables appears to not be statistically significant. Those variables being:

1.  Citric Acid (g/dm^3^)

2.  Total sulfur dioxide (mg/dm^3^)

This strengthens our previous assumption that these variables have a higher chance of being excluded from the "best" regression model later when performing the variable selection process.

<br>

## Inspecting Updated Model using Diagnostic Plots

```{r, results = "hold"}

par(mfrow=c(1,2))
plot(updated_full_model,which=2, main="Figure 5(a):") # QQ Residuals
plot(updated_full_model,which=5, main="Figure 5(b):") # Residuals vs Leverage

```

**INTERPRETATION**

QQ Residuals: Seem to follow along the 45 degree reference line. This shows that under the new model, the residuals are better suited by assuming a normal distribution, which is an ideal condition for regression analysis.

Residuals vs Leverage: There appear to be no points that lie outside the designated Cook's distance contour lines, indicating that there are no overly influential points under the new model.

<br>

# Finding "Best" Regression Model

## Investigating Correlation

```{r, results = "hold"}

# finding correlation values between all variables

correlation_matrix<-round(cor(updated_Dataset),2) 

# visualize correlation in a diagram
par(mfrow=c(1,2))

corrplot::corrplot(correlation_matrix, title="Figure 6(a) : Correlation Plot \nbetween Regressors",mar=c(0,0,3,0))
corrplot(correlation_matrix, method = "number", number.cex = 0.45,title="Figure 6(b) : Correlation Plot \nbetween Regressors",mar=c(0,0,3,0))

```

**INTERPRETATION**

We can separate the interpretation into 2 parts:

1)  Dependent variable with the regressor variables:

By referring to the last row under the correlation plot, we can identify the correlation between the dependent variable against all 11 other independent variables under this dataset. We notice out of all variables, there are two variables that stand out, that being alcohol (vol.%) and also density (g/cm^3^). The exact correlation values are 0.49 and -0.36 respectively and a higher value indicates a stronger linear relationship and the potential to be a valuable predictor of y.

2)  Regressor variables with each other (multicollinearity)

By referring to the upper or lower non-diagonal entries under the correlation plot, we observe that there appears to be very little correlation between the x variables, some of which are even orthogonal (no linear relationship between the regressors) and thus indicating that issues of multicollinearity is minimal under this dataset. In other words, this means that it is likely each predictor provides unique and valuable information in predicting the dependent variable. However, there are two exceptions when observing the correlation plot:

-   Alcohol & Density : correlation value of -0.82

-   Density & Residual sugar : correlation value of 0.84

```{r message=FALSE, warning=FALSE, results="hold"}

plota = ggplot(Dataset, aes(x = alcohol, y = density)) +
  geom_point() +            # Add points
  geom_smooth(method = "lm", se = FALSE, color = "red") + 
  labs(title = "Figure 7(a): Alcohol vs density",
       x = "Alcohol",
       y = "Density") + theme(plot.title = element_text(hjust=0.5))
plotb = ggplot(Dataset, aes(x = residual.sugar, y = density)) +
  geom_point() +            # Add points
  geom_smooth(method = "lm", se = FALSE, color = "red") + 
  labs(title = "Figure 7(b): Residual sugar vs density",
       x = "Residual sugar",
       y = "Density") + theme(plot.title = element_text(hjust=0.5))

require(gridExtra)
grid.arrange(plota,plotb, ncol=2)
```

Both are relatively high in terms of their correlation, thus further investigation into their multicollinearity properties should be investigated using other diagnostics. In this case, we will make use of the "variance inflation factor". Note that we will use the threshold as VIF values $> 5$ being considered to have high multicollinearity problems.

<br>

## Investigating Multicollinearity for Full Model

```{r, results = "hold"}

# investigating multicollinearity under model 

cat("===================================================================================================","\n","\n")
cat("( VIF: updated full model )","\n","\n")

vif_updated_full_model<-vif(updated_full_model)
vif_updated_full_model

cat("===================================================================================================","\n","\n")



```

 

```{r, results="hold"}

# pinpoint key predictors with high vif values

vif_alcohol_full_model <- vif_updated_full_model['alcohol']
vif_density_full_model <- vif_updated_full_model['density']
vif_residual_sugar_full_model <- vif_updated_full_model['residual.sugar']

# constructing table

table_3 <- data.frame(
vif_full_model = c(vif_alcohol_full_model, vif_residual_sugar_full_model, vif_density_full_model ))

#display table 

knitr::kable(table_3, caption = "Table 4: High VIF values under the Updated Full Model ") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

**INTERPRETATION**

Under the updated full model, these three variables exhibit high multicollinearity indications. Thus when building our regression models later, we will need to take into account these values and make appropriate adjustments if necessary.

<br>

## Fitting Simple Linear Model

With the updated dataset, we are now ready to start finding our "best" regression model. We will employ the use of a forward step procedure where we start with including one regressor variable in our model, and work our way up towards the best regression model by adding one regressor variable at a time.

In order to determine the first variable that should be included under the simple linear regression model, we will make use of the variable with the highest correlation value to our dependent variable, i.e. Alcohol (vol.%). Note that from here on out, "best_model_x" will refer to a the fitted model in respect to "x" number of variables, for example best_model_1 refers to 1 predictor being fitted in the model, or basically the best simple linear regression model.

```{r message=FALSE, warning=FALSE, results="hold"}

# fitting the "best" simple linear model

best_model_1<-lm(quality~alcohol,data = updated_Dataset)
summary(best_model_1)

# extracting and finding relevant metrics

Residual_standard_error_best_model_1<-summary(best_model_1)$sigma
R_squared__best_model_1<-summary(best_model_1)$r.squared
best_model_1_AIC<-AIC(best_model_1)
best_model_1_BIC<-BIC(best_model_1)

# displaying results

table_4 <- data.frame(
  Metrics = c("AIC", "BIC", "R-squared", "Residual standard error"),
  updated_full_model = c(updated_full_model_AIC, updated_full_model_BIC, R_squared_updated_full_model, Residual_standard_error_updated_full_model),
  best_model_1 = c(best_model_1_AIC, best_model_1_BIC, R_squared__best_model_1, Residual_standard_error_best_model_1))

# formatting table

table_4$best_model_1 <- format(table_4$best_model_1, digits = 5)
table_4$updated_full_model <- format(table_4$updated_full_model, digits = 5)

# display table of simple model and full model

knitr::kable(table_4, caption = "Table 5: Comparison of Full vs Simple Linear Model ") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

**INTERPRETATION**

When comparing the metrics above with the simple linear model against the new full model, it is apparent that the simple linear model have values that are less ideal, such as a higher AIC and Residual standard error. However, this is to be expected as only one regressor has been added, in theory, as we add more explanatory variables, the metrics will eventually get more and more ideal until it reaches its optimum value stance where we can conclude the best model has been found.

<br>

## Fitting Multiple Linear Model

To decide on the next variables added, we decided to see which of the remaining regressor variables when added to the current model has the best statistical significance ,i.e. the lowest p-value. Then by setting our stopping criterion based on a p-value of 0.05, any variables when added to the model results in a p-value greater than 0.05 will be deemed as non significant and the best model will have been determined.

```{r, results = "hold"}

cortable = as.data.frame(cor(updated_Dataset))
cortablefiltered = subset(cortable, select = quality) 
numbering = seq(12)
names = names(cortable)

data1 = data.frame(cortablefiltered, numbering, names)
data1 = data1[-12,]
data1$quality = abs(data1$quality)

pairs = c(NULL)
naming = names[-12]
naming = naming[-11]
nametest = naming

a = 12
b= 10

for (i in 1:a) {
  pvalue = c(NULL)
  AIC = c(NULL)
  count = c(NULL)
  stop = FALSE
  
  framefull = data1 %>% arrange(desc(quality))
  pairs[1] = framefull[1,3]
  cat("\nquality ~", pairs[i], "+ \n")
  
  for (j in 1:b) {
    formula = as.formula(paste("quality ~",pairs[i]," + ", nametest[j]))
    pvalue[j] = (summary(lm(formula, data = updated_Dataset))$coefficients)[i+2,4]
    AIC[j] = summary(glm(formula, data = updated_Dataset))$aic
    cat("\n\n")
    print(summary(lm(formula, data = updated_Dataset))$coefficients)
    count[j] = j
  }
  
  framefull2 = data.frame(pvalue, nametest, count)
  framefull2 = framefull2 %>% arrange(pvalue)
  nametest = nametest[-framefull2[1,3]]

  pairs[i+1] = paste(c(pairs[i], framefull2[1,2]), collapse = " + ") 
  b = b - 1
  cat("\n", i," iteration done\n")
  
  cat("\nCurrent AIC: ", summary(glm(as.formula(paste("quality ~",pairs[i])), data = updated_Dataset))$aic, "\n\n")
  
  print(AIC)
  
  cat("\nP-values for each of next variables\n")
  print(pvalue)
  print("==========================================================")
  
  if (pvalue[framefull2[1,3]]>0.05){
    stop = TRUE
    formula2 = paste("quality ~", pairs[i])
    print(summary(glm(formula2, data = updated_Dataset)))
    break
  }
  if (stop){break}
}


```

**INTERPRETATION**

By inspection, we can conclude that the our current best fitted model consists of the following variables in the specified order:

1.  Alcohol (vol.%)
2.  Volatile acidity (g(acetic acid)/dm^3^)
3.  Residual sugar (g/dm^3^)
4.  Free sulfur dioxide (mg/dm^3^)
5.  pH level
6.  Density (g/cm^3^)
7.  Sulphates (g(potassium sulphate)/dm^3^)
8.  Fixed acidity (g(tartaric acid)/dm^3^)
9.  Chlorides (g(sodium chloride)/dm^3^)

For further investigation and verification, we will also see how the other values (AIC, BIC, R-squared, Residual standard error) change as each new variable is fitted, if it is consistent with our findings, we should observe a trend where each of the values become more ideal with every variable included at each step.

```{r, results = "hold"}

# fitting each of the models as best_model_x

# Create an empty list to store the models

best_models <- list()

# Loop through each element in pairs

for (i in 1:9) {
formula <- as.formula(paste("quality ~", pairs[i]))
best_models[[i]] <- lm(formula, data = updated_Dataset)

assign(paste0("best_model_", i), best_models[[i]])
}

```

```{r, results = "hold"}

# finding the relevant values for each of the models

model_names <- paste0("best_model_", 1:9)

# Loop through each model

for (i in 1:9) {
model <- get(model_names[i])

assign(paste0("Residual_standard_error_", model_names[i]), summary(model)$sigma)
assign(paste0("R_squared_", model_names[i]), summary(model)$r.squared)
assign(paste0(model_names[i], "_AIC"), AIC(model))
assign(paste0(model_names[i], "_BIC"), BIC(model))}

table_5 <- data.frame(
  Metrics = c("AIC", "BIC", "R-squared", "Residual standard error"))

# Loop through each model to extract metrics and store them in the data frame

for (i in 1:9) {
model <- get(model_names[i])
  
aic_value <- AIC(model)
bic_value <- BIC(model)
r_squared_value <- summary(model)$r.squared
residual_standard_error <- summary(model)$sigma

table_5[paste0("best_model_", i)] <- c(aic_value, bic_value, r_squared_value, residual_standard_error)}

# formatting the table

for (i in 1:9) {
  column_name <- paste0("best_model_", i)
  table_5[[column_name]] <- format(table_5[[column_name]], digits = 5)}

# displaying the table

knitr::kable(table_5, caption = "Table 6: Comparison of all Fitted Models") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

**INTERPRETATION**

We can observe that for every new variable added, the metric values become more ideal for every model until best_model_9. To further investigate the quality of our model, we check on the Mallows $Cp$ value.

<br>

## Checking Mallows **Cp**

```{r, results = "hold"}

# checking Mallows Cp for the current best model fitted

ols_best_model_9<-ols_mallows_cp(best_model_9,updated_full_model)
ols_best_model_9

```

**INTERPRETATION**

Under the model, we can observe that the Mallows $Cp$ value of 9.18884 which is seemingly lower than th ideal value of $k+1=p=12$ which indicates that the the current model might be underfitting the data.

<br>

## Reinvestigating Multicollinearity Properties

Despite all the metrics pointing towards the conclusion that we have fitted the best model, it is also essential to consider multicollinearity in addition to traditional model evaluation metrics such as R-squared, AIC, BIC, and residual standard error. Hence we need to evaluate the presence of multicollinear issues for each model starting from the model including two regressor variables and so forth. Again, we will make use of the VIF values to investigate multicollinearity.

```{r, results = "hold"}

# seeing vif for each model

cat("===================================================================================================","\n","\n")
cat("( VIF: 2 Predictor Model )","\n","\n")
car::vif(mod=best_model_2)%>%sort()
cat("\n")

cat("===================================================================================================","\n","\n")
cat("( VIF: 3 Predictor Model )","\n","\n")
car::vif(mod=best_model_3)%>%sort()
cat("\n")


cat("===================================================================================================","\n","\n")
cat("( VIF: 4 Predictor Model )","\n","\n")
car::vif(mod=best_model_4)%>%sort()
cat("\n")


cat("===================================================================================================","\n","\n")
cat("( VIF: 5 Predictor Model )","\n","\n")
car::vif(mod=best_model_5)%>%sort()
cat("\n")


cat("===================================================================================================","\n","\n")
cat("( VIF: 6 Predictor Model )","\n","\n")
car::vif(mod=best_model_6)%>%sort()
cat("\n")

cat("===================================================================================================")

```

**INTERPRETATION**

We have observed that each model starting from 2 predictor variables to 5 predictor variables have VIF values $< 5$ which shows no potential multicollinearity problems. However, it is only when the 6th predictor, density, was added where we observe that there exists VIF values that exceed 5, specifically

-   Density : 14.526068

-   Residual Sugar : 6.729140

-   Alcohol : 5.488864

This observation is consistent with when we investigated multicollinearity for the updated full model where density had a high correlation value with both alcohol alongside residual sugar. It is with reasonable assumption that density is the root predictor in which is resulting in the presence of multicollinear issues and thus, provides justification into removing this variable from our current best model.

<br>

## Fitting "Best" Model without Density Variable

Now, we will observe how the different metrics have changed after removing the density predictor variable.

```{r, results = "hold"}

# fitting best model without density

updated_model<-lm(quality~ alcohol + volatile.acidity + residual.sugar + free.sulfur.dioxide + pH + sulphates + fixed.acidity + chlorides, data=updated_Dataset)
summary(updated_model)

```

```{r, results = "hold"}

# recheck multicollinearity

cat("===================================================================================================","\n","\n")
cat("( VIF: Updated Best model without density variable )","\n","\n")
car::vif(mod=updated_model)%>%sort()
cat("\n")

cat("===================================================================================================")

```

**INTERPRETATION**

After removing the density variable, it aligns with our assumption where the model should no longer have any indication of multicollinearity issues as evidenced with all the VIF values being below the threshold of 5 and the maximum value being only 1.689441 in respect to the alcohol variable. However, we should reinvestigate how this removal has affected the other metrics which can help in determining the overall quality of the model.

```{r, results ="hold"}

# extracting the relevant metrics

Residual_standard_error_updated_model<-summary(updated_model)$sigma
R_squared_updated_model<-summary(updated_model)$r.squared
updated_model_AIC<-AIC(updated_model)
updated_model_BIC<-BIC(updated_model)
ols_updated_model<-ols_mallows_cp(updated_model,updated_full_model)

# constructing the table

table_6 <- data.frame(
  Metrics = c("AIC", "BIC", "R-squared", "Residual standard error", "Mallows Cp"),
  initial_best_model = c(best_model_9_AIC, best_model_9_BIC, R_squared_best_model_9, Residual_standard_error_best_model_9,ols_best_model_9),
 updated_best_model = c(updated_model_AIC, updated_model_BIC, R_squared_updated_model, Residual_standard_error_updated_model,ols_updated_model))

# formatting the table

table_6$initial_best_model <- format(table_6$initial_best_model, digits = 5)
table_6$updated_best_model <- format(table_6$updated_best_model, digits = 5)

# displaying the table

knitr::kable(table_6, caption = "Table 7: Comparison of Best Model vs Model without Density Variable ") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

**INTERPRETATION**

From our analysis, we observe that the removal of the density variable has actually led to these metric values exhibiting less ideal values instead. In particular, we can observe that the Mallows $Cp$ is much higher than the number of predictors in the model which indicates that under this new model, it displays biasness and does not fit the data well. In such cases, the benefits of improved predictive accuracy and better model performance may outweigh the drawbacks of multicollinearity. Hence, while multicollinearity can complicate the interpretation of individual coefficients, the overall enhancement in model quality justifies retaining the collinear variable

<br>

## Investigating Problematic Data Points

To further improve the model we should reinvestigate problematic points under the best model. It is important to note that even after removing some influential points during the initial fitting of the updated full model, it is possible to encounter new influential points under the subsequent models as we fit different models with varying numbers and combinations of predictors. Each model may highlight different data points as problematic, necessitating a thorough review and adjustment to ensure the final model's robustness and accuracy.

```{r, results = "hold"}

# studentized residuals 

studentized_residuals_best_model_9<-studres(best_model_9)

# cooks distance 

cd_best_model_9<-cooks.distance(best_model_9)

# leverage

hat_values_best_model_9<-hatvalues(best_model_9)

# plot for visualization

par(mfrow=c(1,3))

plot(studentized_residuals_best_model_9, main="Figure 8(a): Studentized residuals \nvisualization", ylab="Studentized Residuals",xlab="Observation point")
abline(h=3,col="red")
abline(h=-3,col="red")

plot(cd_best_model_9, main="Figure 8(b): Cook's distance \nvisualization", ylab="cooks distance",xlab="Observation point")
abline(h = 4/(nrow(updated_Dataset)), col = "red")

plot(hat_values_best_model_9, main="Figure 8(c): Leverage \nvisualization",ylab="leverage", xlab="Observation point")
abline(h= 2 * (length(best_model_9$coefficients)) / nrow(updated_Dataset),col ="red")



```

```{r, results = "hold"}


high_studentized_residual_points_2<-which(studentized_residuals_best_model_9>(abs(3)))
high_leverage_points_2<-which(hat_values_best_model_9>(2*(length(best_model_9$coefficients))/nrow(updated_Dataset)))
high_cooks_distance_points_2<-which(cd_best_model_9>(4/nrow(updated_Dataset)))

all_potential_issues_2<-unique(c(high_studentized_residual_points_2,high_leverage_points_2,high_cooks_distance_points_2))

# create finalized dataset 

finalized_Dataset<-updated_Dataset[-all_potential_issues_2, ]

# fitting finalized best model

finalized_best_model<-lm(quality~alcohol + volatile.acidity + residual.sugar + free.sulfur.dioxide + pH + density + sulphates + fixed.acidity + chlorides,data=finalized_Dataset)
finalized_full_model<-lm(quality~.,data=finalized_Dataset)

summary(finalized_best_model)

# comparing initial best model with finalized model

Residual_standard_error_finalized_best_model<-summary(finalized_best_model)$sigma
R_squared_finalized_best_model<-summary(finalized_best_model)$r.squared
finalized_best_model_AIC<-AIC(finalized_best_model)
finalized_best_model_BIC<-BIC(finalized_best_model)
ols_finalized_best_model<-ols_mallows_cp(finalized_best_model,finalized_full_model)

# constructing the table

table_7 <- data.frame(
  Metrics = c("AIC", "BIC", "R-squared", "Residual standard error", "Mallows Cp"),
  initial_best_model = c(best_model_9_AIC, best_model_9_BIC, R_squared_best_model_9, Residual_standard_error_best_model_9,ols_best_model_9),
  
 finalized_best_model = c(finalized_best_model_AIC, finalized_best_model_BIC, R_squared_finalized_best_model, Residual_standard_error_finalized_best_model,ols_finalized_best_model))

# formatting the table

table_7$initial_best_model <- format(table_7$initial_best_model, digits = 5)
table_7$finalized_best_model <- format(table_7$finalized_best_model, digits = 5)

# displaying the table

knitr::kable(table_7, caption = "Table 8: Comparison of Initial vs Finalized Best Model ") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

**INTERPRETATION**

Initially, our best model had a Mallows' $Cp$ value of 9.18. After removing the influential points and recalculating, the $Cp$​ value changed to 11.72 which is much closer to the ideal value of 12 which indicates that model adequately explains the variance in the response variable. Additionally, when examining other metrics, the updated model shows a clear improvement: the AIC and BIC values are lower, indicating a better balance between model fit and complexity; and additionally the R-squared value is higher alongside a smaller residual standard error value. These improvements in key performance indicators justify that the updated model, is overall a better fitted model.

<br>

# Verification of Results

Now that we have identified our best model, we can make further investigation and verification using inbuilt functions designed for model selection. Specifically, we will utilize forward stepwise selection, backward stepwise selection, all-subsets regression (also known as all possible subsets or best subsets regression) and the best subsets regression. By comparing our manually selected best model with those identified by these automated procedures, we can validate our findings and ensure that we have indeed chosen the most accurate and efficient model.

```{r, results = "hold"}

# fitting null model necessary for the following model selection techniques

nullmodel<-lm(quality~1,data=updated_Dataset)
summary(nullmodel)

```

## All Subsets Regression

```{r, results = "hold"}

# Using all subsets selection

best_step_model<-step(nullmodel,scope=list(lower=nullmodel, upper=updated_full_model,direction = "both"))
summary(best_step_model)

```

## Forward Stepwise Selection

```{r, results = "hold"}

# Using forward stepwise selction

forward_model<-step(nullmodel,scope=list(lower=nullmodel,upper=updated_full_model,direction="forward"))
summary(forward_model)

```

## Backward Stepwise Selection

```{r, results = "hold"}

# Using forward stepwise selction

backward_model<-step(updated_full_model,direction = "backward")
summary(backward_model)
```

## Best Subsets Regression

```{r, results = "hold"}

# Using Best Subsets Regression

best_subsets_model<-regsubsets(quality~.,data=updated_Dataset, nbest=1,nvmax=11) 
plot(best_subsets_model, main= "Figure 9: BIC against Regressors")

```

**INTERPRETATION**

After conducting best subsets regression and stepwise regression, we found that all results interestingly enough produced the same best model as our previously identified one. Both methods systematically evaluated different combinations of predictors and, despite their different approaches, consistently pointed to the same model as the optimal choice. This convergence of results allows for a greater confidence in the accuracy and effectiveness of our final best model.

<br>

## Justify the Inclusion of Variable

As such, we can say that the best model fitted according to our steps is shown as follows:

<br>

$$
y=205.698+0.110x_1-1.671x_2+0.092x_3+0.004x_4+1.110x_5-207.210x_6+0.751x_7+0.145x_8-4.736x_9
$$

$Where:$

$x_1=alcohol$

$x_2=volatile\ acidity$

$x_3=residual\ sugar$

$x_4=free\ sulfur\ dioxide$

$x_5=pH\ level$

$x_6=density$

$x_7=sulphates$

$x_8=fixed\ acidity$

$x_9=chlorides$

<br>

1.  Volatile and Fixed Acids

-   Responsible for the wine’s freshness and vitality
-   Good acid balance - zesty, crisp character.
-   Good structure and aging.

Exclusion of Citric acid : Not significant - Weak organic acid (Not much effect as compared to other fixed acids) - Microbial instability - Growth of unwanted microbes Citation: (Jamescharleswine, 2023)

2.  Residual Sugars

-   Natural grape sugars leftover.
-   Affects a wine’s sweetness.
-   Sugars that are non-fermentable and fermentable pentoses.
-   Sweet Wine: 45 g/l. Citation: (Wu, 2020)

3.  Alcohol Content

-   Subtle impression of sweetness.
-   Hint of bitterness
-   Less Content – Flat and Dull Citation: (LiquidLINE Blog – Things That Affect the Wine Quality, n.d.-b)

4.  Chlorides

-   Saltiness of a wine Citation: (Application Note #105 -Chloride in Wine by Titration, n.d.)

5.  Density

-   Higher density tend to have a richer mouthfeel and more full-bodied character.
-   Often exhibit more pronounced flavors and aromas due to the higher concentration of solutes. Citation: (A Guide to Understanding Wine Density and Concentration \| All Wines of Europe, 2023)

6.  Total and Free Sulphur Dioxide

-   Preservative
-   Continuously monitor sulfur dioxide levels until wine is bottled.
-   Act as a cushion for FSO2.
-   FSO2 is lost, chemical equilibrium in the wine shifts so that some of the S02 can be released to its free state. Citation: (Admin, 2018) & (Comfort, 2008)

7.  pH Level

-   Low pH wines will taste tart and crisp
-   Higher pH wines are more susceptible to bacterial growth
-   3.0 to 3.4 for white wines.
-   3.3 to 3.6 for red wines. Citation: (Edison, 2022)

8.  Sulphates

-   Potassium Metabisulfite (derivatives of SO2)
-   Antioxidant to maintain the wine’s colour, flavour, and aroma.
-   Also a result of the lightly oxidizing environment represented by wooden casks.
-   Low sulfate levels could attribute to wine being stored in stainless steel containers. Citation: (Contributors, 2021)

<br>

# Conclusion and Recommendation

Based on the results obtained, the following may contribute to higher wine quality:

a)  High alcohol level (over 10)

By harvesting grapes later in the season, particularly 1-2 months after the regular harvest time will allow them to accumulate more sugars which will result in higher alcohol content. This is a result of the yeast molecules having more sugar available to convert to alcohol during fermentation (Puckette, 2016). Other than that, winemakers can utilize cryoextraction, which involves freezing grapes and removing water in the form of ice to increase alcohol content in wine. This method does so by further concentrating sugars (Tatum, 2016).

b)  Low density (under 0.998)

In order to achieve low density wines, winemakers should maintain optimal fermentation temperatures (32°C to 35°C) to enable the fermentation process to be completed efficiency. This will reduce residual sugars which will lead to a lower wine density (Liszkowska & Berlowska, 2021). Besides that, winemakers can blend the wine with another wine with a lower alcohol content or even water, although it is strictly regulated and rarely used, to reduce alcohol content which will in turn reduce density (Sullivan, 2023).

c)  Increased pH (over 3)

In order to raise the alkalinity of wine, winemakers can add chemical compounds, such as potassium carbonate and calcium carbonate. They help neutralize acidity by reacting with the tartaric acid in wine to precipitate calcium tartrate or potassium bitartrate out of the solution (Pambianchi, n.d.). Malolactic fermentation (MLF) can also help increase the pH of wines. This method is a secondary fermentation process where lactic acid bacteria convert malic acid into lactic acid, which is a weaker acid which reduces the acidity of the wine. This also helps to add complexity and softness to the wine, especially red wine and some white wines, like Chardonnay (Lonvaud-Funel, 2022).

By implementing the above techniques, Vinho Verde will be able to meet consumer expectations on a more prestigious level and maintain their reputation as the top wine brand in Portugal.

<br>

# References

-   Admin. (2018, February 27). *Total Sulfur Dioxide – Why it Matters, Too!* Midwest Grape and Wine Industry Institute. <https://www.extension.iastate.edu/wine/total-sulfur-dioxide-why-it-matters-too/>

-   *A Guide to Understanding Wine Density and Concentration \| All Wines of Europe*. (2023, July 17). <https://allwinesofeurope.com/a-guide-to-understanding-wine-density-and-concentration/>

-   *Application Note #105 -Chloride in Wine by Titration*. (n.d.). <https://mantech-inc.com/wp-content/uploads/2014/07/105-Chloride-in-Wine-by-Titration.pdf>

-   Comfort, S. (2008, December 5). *About Acidity and Adding Acid to Must/Wine \| MoreWine*. Morewinemaking.com. <https://morewinemaking.com/articles/Acidifying_must>

-   Contributors, W. E. (2021, June 22). *What to Know About Sulfites in Wine*. WebMD. <https://www.webmd.com/diet/what-to-know-sulfites-in-wine>

-   Corduas, M., Cinquanta, L., & Ievoli, C. (2013). The importance of wine attributes for purchase decisions: A study of Italian consumers’ perception. *Food Quality and Preference*, *28*(2), 407–418. <https://doi.org/10.1016/j.foodqual.2012.11.007>

-   Edison, T. (2022, January 26). *Is Wine Acidic or Alkaline? [Guide to pH in Winemaking]*. Wineturtle.com. <https://wineturtle.com/wine-acidic-alkaline-basic/>

-   Jamescharleswine. (2023, September 26). *The role of acids, sugars, and tannins in wine quality*. James Charles Winery. <https://jamescharleswine.com/the-role-of-acids-sugars-and-tannins-in-wine-quality/#:~:text=Role%3A%20Acids%20are%20essential%20in,tartaric%2C%20malic%2C%20and%20citric>

-   *LiquidLINE Blog – Things that Affect the Wine Quality*. (n.d.). OPSIS LiquidLINE. <https://www.liquidline.se/blog/things-that-affect-the-wine-quality/#:~:text=The%20alcohol%20in%20wine%20gives,much%20and%20not%20too%20little>.

-   Liszkowska, W., & Berlowska, J. (2021). Yeast Fermentation at Low Temperatures: Adaptation to Changing Environmental Conditions and Formation of Volatile Compounds. Molecules, 26(4). <https://doi.org/10.3390/molecules26041035>

-   Lonvaud-Funel, A. (2022). Malolactic fermentation and its effects on wine quality and safety. Elsevier EBooks, 105–139. <https://doi.org/10.1016/b978-0-08-102065-4.00008-0>

-   Marianthi Basalekou, Panagiotis Tataridis, Georgakis, K., & Christos Tsintonis. (2023). Measuring Wine Quality and Typicity. *Beverages*, *9*(2), 41–41. <https://doi.org/10.3390/beverages9020041>

-   Pambianchi, D. (n.d.). Monitoring & Adjusting pH. WineMakerMag.com. <https://winemakermag.com/technique/1650-monitoring-adjusting-ph>

-   Puckette, M. (2016, October 3). Late Harvest Wines and Why They’re Awesome. Wine Folly. <https://winefolly.com/tips/late-harvest-wines-and-why-theyre-awesome/>

-   Sullivan, S. P. (2023, May 4). Basics: The Why, When and How of Wine Blending \| Wine Enthusiast. Www.wineenthusiast.com. <https://www.wineenthusiast.com/culture/wine/wine-blending/>

-   Tatum, M. (2024, May 16). What is Cryoextraction? (with pictures). DelightedCooking. <https://www.delightedcooking.com/what-is-cryoextraction.htm>

-   Wu, S. (2020, July 16). *What is residual sugar in wine? – Ask Decanter*. Decanter. <https://www.decanter.com/learn/residual-sugar-46007/#:~:text=The%20amount%20of%20residual%20sugar,be%20consumed%20by%20the%20yeast>

 
